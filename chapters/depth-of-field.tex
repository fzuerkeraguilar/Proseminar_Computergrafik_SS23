\chapter{Depth-of-Field}
As discussed in chapter \ref{ch:background} virtual cameras are able to take perfect color samples from every angle and thus create an image with infinite \gls{dof}.
However without expensive operations, like the simulation of a lens with path tracing, a virtual camera is only able to create perfect images.
As such the difficulty in simulating depth of field in real-time lies in combining images with infinite \gls{dof} into good approximation of real camera images.
The ideal method would be able to produce close to ray-tracing accuracy while having real-time performance.
To approach this ideal several approaches have been developed, which will be presented in the following.

\section{Accumulation buffers}
The earliest method to generate a depth-of-field effect using traditional rasterization hardware was presented by Haeberli and Akeley \cite{Haeberli.1990}.
Their method introduces the use of an accumulation buffer to the rendering pipeline to simulate multiple effects, including motion blur and \gls{dof}.
The accumulation buffer is a region of memory used by the rendering hardware to accumulate and average the results of multiple rendering passes.
To simulate a depth-of-field effect the following steps are iterated:
\begin{enumerate}
    \item \textbf{Jittering:} The camera position is slightly disturbed. Commonly done by offsetting the principal point of the camera, but keeping the camera pointed at the same point in the scene
    \item \textbf{Rendering:} An image is created using the traditional rasterization pipeline
    \item \textbf{Accumulation:} The rendered image is added to the accumulation buffer.
\end{enumerate}
The jittering of the camera simulates the size of the aperture of a real image system.
The images are then averaged to approximate the integration of all light paths hitting a sensor pixel.
Consequently, out of focus objects will occupy varying positions in the rendered images and will be diffused over their \gls{coc} in the final image.
This method therefore converges on the correct solution.

\begin{wrapfigure}{R}{0.4\linewidth}
\begin{center}
    \includegraphics[width=0.4\textwidth]{images/sample-locations.png}
\end{center}
\caption{Position of sample location used to sample the aperture of the camera lens \cite{Haeberli.1990}}
\label{fig:sample-pos}
\end{wrapfigure}

The specific approach to jittering the camera is subject to the individual implementation, but should move the camera parallel to the image plane.
Sampling points in a circular area may be used to simulate a circular aperture, but a hexagonal or octagonal sampling shape may be preferred for mechanical apertures.
Further customization can be done by adjusting the distribution of points to achieve a specific \gls{psf}.
As such this approach great artistic flexibility.

To hide the individual frames a minimum density of samples must be taken to sufficiently cover the biggest \gls{coc} present in the image.
As a result, the amount of work needed to achieve acceptable results scales linearly with the \textbf{area} of the biggest \gls{coc} created by the scene.
Acceptable results are typically achieved with a single pass per 4 pixel area of the \gls{coc}.
This still results in 50 passes at a \gls{coc} with a 8 pixel radius \cite{Demers.2005}.
Thanks to the use of a traditional rasterization pipeline, this method is still often faster than ray-traced approaches.
However the high increase in the rendering workload excludes its use except in the most CPU-bound real-time rendering workloads or for reference images.

\section{Single-Layer approaches}
To improve on the performance of accumulation buffers, several techniques have been developed, that limit themselves to a single rendering pass.
In addition to the color information in the frame buffer, all these techniques store the depth of each pixel in a separate z-buffer.
The main way the following methods differ is in their use of the given data to create the final image.
Due to the limitation on the information provided, all of the following methods exhibit some artifacts, that are accepted in exchange for the higher performance they offer.

The single-layer approaches can be categorized into two sub-categories based on the direction of how the final color for each pixel is calculated.

\subsection{Scattering methods}
The first single-layer method to generate depth of field, commonly referred to as \textit{splatting}, or \textit{forward-mapped z-buffer \gls{dof}} was introduced by Potmesil et al. \cite{Potmesil.1981}.
It relies on the fact that the image of pinhole camera stores the light intensity for \textbf{most} directions that hit the sensor.
With the additional information offered by the z-buffer, we can calculate the \gls{coc} of each light ray.
To efficiently calculate the final color of any pixel, a sprite with the calculated \gls{coc} size is placed at its position.
The color of the sprite is taken from the pixel in the original image, while its alpha value is inversely proportional to the area of the calculated \gls{coc}.
Similarly to the accumulation buffer method, the shape and intensity distribution of the sprite can be modified to simulate a specific \gls{psf}.
The sprite is then placed at the original distance and second rendering pass is used to generate the final image.

The resulting image is a very close approximation of an image generated by the accumulation buffer method.
This method's main visual inaccuracy lies in its inability to simulate partial occlusion.
In realistic cameras with a real aperture, light may enter the aperture and hit the image sensor, which would not be visible to a pinhole camera.
This effect is illustrated in figure \ref{fig:partial-occlusion}.
While this effect is barely noticeable in static scenes, it causes bright objects to suddenly appear when coming into view behind other objects \cite{Demers.2005}.

Additionally, as this method relies on the interaction of transparent sprites, it requires the sorting of each sprite from back to front to ensure accurate transparency.
This log-linear time complexity in regard to the resolution is the main drawback of this method.
Although the performance may not meet the stringent real-time application requirements of video games, the superior image quality it offers renders it an appealing option for previews of renders.
However, it's worth noting that real-time performance can be realized when employing commutative blending.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{images/partial-occlusion.png}
    \caption{An object is partially visible through a finite aperture \cite{Barsky.2003b}}
    \label{fig:partial-occlusion}
\end{figure}

Lee et al. \cite{Lee.2008} present a modified version of this algorithm that splits the scene into three layers.
Splatting is done via commutative blending onto three separate layers.
A second rendering pass of hidden scene elements is done to alleviate the partial occlusion problems of the traditional method.

\subsection{Gathering methods}
The second single-layer method was introduced by Rokita et al. \cite{Rokita.1993} and gathers the color information for each pixel of the final image by consulting the image of a pinhole camera.
It is also commonly referred to as \textit{reverse-mapped z-buffer \gls{dof}} \cite{Gilham.2007}.

The simplest implementation of this approach is achieved by generating a mip-map of the rendered screen texture.
Then the \gls{coc} is calculated for each pixel and is used to perform trilinear interpolation to find the final color value.
To access the depth buffer was an issue in early graphics APIs and early GPUs and had to be manually created.
However since the introduction of OpenGL 2.0 and DirectX 9 and compatible hardware, access to depth buffer information is widely supported.
As this process is easily parallelized per pixel and has hardware support on modern GPUs, it is very performant on modern graphics hardware.
However the simple implementation generates multiple objectionable visual artefacts, that make the simple implementation unsuitable for most applications \cite{Demers.2005, Hammon.2008}.

The worst artifact is caused by the single depth lookup per pixel.
In the case of a real camera system, objects and by extension their outline are blurred.
A point at the edge of an out of focus object will spread out over the focused background.
This causes a smooth transition from blurry objects in the foreground to the focused objects.
Taking a single depth lookup per pixel will only blur the image section containing the out of focus object and its outline.
As such a hard boundary between the foreground and the focused part of the image will appear in the final image.
This artifact can be seen in figure \ref{fig:depth-discontinuity} and is commonly refereed to as a depth discontinuity \cite{Demers.2005}.

%\begin{wrapfigure}{R}{0.5\linewidth}
%\begin{center}
%    \includegraphics[width=0.48\textwidth]{images/depth-discontinuity.jpg}
%\end{center}
%\caption{Depth Discontinuity causes hard edges around blurred objects. \cite{Demers.2005}}
%\label{fig:depth-discontinuity}
%\end{wrapfigure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/depth-discontinuity.jpg}
    \caption{Depth discontinuity causes hard edges around blurred objects. \cite{Demers.2005}}
    \label{fig:depth-discontinuity}
\end{figure}

Hammond \cite{Hammon.2008} presents an approach to resolve this issue and generate a more natural \gls{dof} for foreground objects.
A full-screen pass that calculates the \gls{coc} for each foreground object is calculated and stored as a texture.
To calculate the \gls{coc} equation \ref{eq:coc-expanded} may be used, but other options are available for more artistic control.
Especially in first-person games it is often desirable for the player's weapons or other important objects to have a different level of \gls{dof}
To achieve this Hammond Jr. proposes using negative depth values to indicate pixel containing the view model.
Pixel in focus or sky are assigned a \gls{coc} of zero.
This depth texture is first downsampled to one-fourth resolution along both axis for better performance.
The downsampled texture is then blurred using a Gaussian blur to eliminate edges between foreground and focused and background objects.
The final \gls{coc} value used by this method is given by:
\begin{align}
    D = 2 \cdot max(D_0, D_B) - D_0.
\end{align}
Where $D_0$ represents the downsampled depth-texture and $D_B$ represents the blurred depth-texture for each pixel.
A final full-screen pass is finally done to apply a variable width blur by interpolating between the original pinhole image and downsampled versions of it.
Lee et al. \cite{Lee.2009} improve on Hammond's method by using non-linear interpolation during the blurring step.
However both methods still suffer from the fundamental limitations of this approach.
As such they both blur object behind transparent foreground objects and struggle with high blur radii.

Another artefact exhibited by image-space techniques are bilinear interpolation artefacts of the magnified layers.
They are a result of the trilinear filtering used and cause a stair-stepped pattern to appear in blurred objects, as can be seen in figure \ref{fig:trilinear-filtering}.
To alleviate these artifacts a different downsampling method should be used.
Another option is to apply a Gaussian blur to the downsampled images to simulate a more realistic blur.
Again, different kernels may be chosen to simulate a particular \gls{psf}.
However for optimal performance a separable kernel should be used \cite{Demers.2005, Gilham.2007}.

%\begin{wrapfigure}{R}{0.5\linewidth}
%\begin{center}
%    \includegraphics[width=0.48\textwidth]{images/trilinear-filtering.jpg}
%\end{center}
%\caption{Magnification Artifacts with Trilinear Filtering \cite{Demers.2005}}
%\label{fig:trilinear-filtering}
%\end{wrapfigure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/trilinear-filtering.jpg}
    \caption{Magnification artifacts with Trilinear Filtering \cite{Demers.2005}}
    \label{fig:trilinear-filtering}
\end{figure}

The final artefact created by these approaches is also caused at the border between foreground and in focus objects.
As the image of blurred objects is blurred indiscriminately, the blurring process will also use color information of in focus areas.
As such focused image areas will spread into blurred areas, as seen in figure \ref{fig:pxl-bleeeding}.
This artefact is called \textit{intensity-} or \textit{pixel-bleeding} \cite{Demers.2005}.
One approach to reduce these artefacts proposed by Lee et al. \cite{Lee.2009} is the use of an \textit{anisotropic mipmapping} scheme to preclude the effects of adjacent pixel.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/pixel-bleeding.jpg}
    \caption{Pixel-bleeding artefacts of in focus tank barrel \cite{Demers.2005}}
    \label{fig:pxl-bleeeding}
\end{figure}

\section{Multi-Layer approaches}
Multi-layer approaches have been developed to address the problem of intensity leakage and the lack of partial occlusion of single-layer approaches.
In multi-layer approaches the rendered pin-hole image is first decomposed into several sub-images each representing a layer in the scene.
The layers are then separately blurred and recombined at the end.
How the image is separated and how the sub-images are combined are the main points of difference between different methods.
In the following we will present a method presented by Kraus and Strengert \cite{Kraus.2007}, which builds upon a method presented by Barsky et al. \cite{BrianA.Barsky.2005}.

First a normal pinhole image with according depth information is rendered and stored as textures.
This image is then split into multiple sub-images denoted by $I^{(i)}$.
Each layer is initialized to the input pinhole image and its depth map.
For each layer a value $z^{(i)}$ is defined by:
\begin{align}
    z^0 &= z_{focal} \\
    z^i &= \frac{z_{focal}}{1 - r^{(i)}_{pix} / r^{(z \rightarrow \infty)}_{pix}}
\end{align}
Where by $r^{(i)}_{pix}$ represents the blur radius given by the blur method for each layer.
Negative values of $i < 0$ represent the foreground, conversely $i > 0$ represent the background.
Due to a following step, all pixel with depth values of $z < z^{(i+2)}$ for layer $I^{(i)}$ are discarded and set to transparent black.\cite{Kraus.2007}

In the next step the discarded foreground pixel of each layers are disoccluded.
This step is mostly done to alleviate the problem of intensity leakage.
A method of GPU-based pyramidal interpolation presented by Strengert et al. \cite{Strengert.2006} is used to interpolate color and depth information across discarded pixel.
To generate information for the missing pixel, first the available pixel and their alpha values are averaged onto a texture with half resolution in both dimensions, similar to the generation of mip-maps.
This is done until a 1x1 resolution is reached.
In the synthesis step of this method, each layer is searched for pixel with missing color information (i.e. with alpha value of 0).
Any such pixel found is given the color of the layer directly above.
This is repeated until the bottom layer is reached, where all RGB-components are divided by their alpha value to get their final color.
A schematic representation of this process is seen in figure \ref{fig:pyramidal-interpolation}.
This offers up new color information for the blurring step and prevents the pixel-bleeding artefacts of caused by closer layers as seen in figure \ref{fig:pxl-bleeeding}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/pyramidal-interpolation.png}
    \caption{Pyramidal Interpolation of missing pixel.\cite{Kraus.2007}}
    \label{fig:pyramidal-interpolation}
\end{figure}

The main difficulty of multi-layer approaches lies in hiding the boarders between individual layers.
To address this issue, this method "matts" each layer $I^{(i)}$ using a piecewise linear function $\omega^{(i)}(z)$ seen in figure \ref{fig:matt-func}.
The color and the alpha components are multiplied by $\omega^{(i)}(z)$.
Thus $\omega^{(i)}(z)$ can be viewed as a weighting function between layers and contributes to a smooth transition between layers in the final blending.
This step must be done before the blurring of individual layers to ensure the blurring step does not use color information of layers further back \cite{Kraus.2007}.

\begin{wrapfigure}{R}{0.5\linewidth}
\begin{center}
    \includegraphics[width=0.48\textwidth]{images/matt-function.png}
\end{center}
\caption{The matting function $\omega^{(i)}(z)$ for sub-image $I^{(i)}$.\cite{Kraus.2007}}
\label{fig:matt-func}
\end{wrapfigure}

In this implementation of the method the blurring of the sub-images is again done via the pyramidal method introduced by Strengert et al. \cite{Strengert.2006}.
To generate a blur of width $2^l$ the original image is copied in to level $G_0$ of the pyramid and levels $G_1$ to $G_l$ are computed.
The synthesis of the final blurred sub-image is done as discussed above.
Kraus et al. propose the use of a scaled blur radius of
\begin{align}
    r^{(0)}_{pix} &= 0 \\
    r^{(i)}_{pix} &= 2.0 \cdot 0.85 \cdot 2^{|i| - 1}
\end{align}
to adjust for the increased perceived blurriness created by the influence of the neighboring layers introduced in the matting step.
Finally the individual sub-images are blended back to front using traditional alpha blending over a buffer initialized to black \cite{Kraus.2007}.

The presented technique provides enhanced image accuracy over single-layer methods, with particular benefits in addressing depth discontinuities and pixel-bleeding.
This is achieved through the allocation of individual pixels to sub-layers based on their differing depths, thereby reducing interference between pixels.
However, it's essential to consider the performance implications of the multi-layer approach, as it necessitates substantial memory performance and capacity.
This is due to the need for concurrent storage and modification of each layer, along with their corresponding pyramids of downscaled layers.
This also hints at the main drawback of multi-layer methods, namely their performance scaling with the size of the largest \gls{coc}.
As the radius of \gls{coc} expands, the layer count must similarly increase to maintain the image quality.
Consequently, performance declines with larger CoCs, reducing the overall effectiveness of this technique.